{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b4274c-3549-495b-a650-336cc0fd174c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700cbfbb-8264-44d1-ad07-53abbdabcde0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d23f90-a1ae-4c80-b7c5-3a43964d8b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1405569601980114#setting/sparkui/1026-211846-nmdz3z69/driver-7259500833320443258\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1405569601980114#setting/sparkui/1026-211846-nmdz3z69/driver-7259500833320443258\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark_ln\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228494c3-e2a4-457a-9a5e-75a2f728b416",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. The map() in pyspark is a transformation function that is used to apply a function(lambda) to each element of an RDD and return a new RDD consisting of the result.\n",
    "\n",
    "2. DataFrame doesn’t have map() transformation to use with DataFrame; hence, you need to convert DataFrame to RDD first.\n",
    "\n",
    "3. PySpark DataFrame doesn’t have map() transformation to apply the lambda function, when you wanted to apply the custom transformation, you need to convert the DataFrame to RDD and apply the map() transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ee555e-b50e-4885-8748-fdd9c9a524b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: ['Project',\n 'Gutenberg’s',\n 'Alice’s',\n 'Adventures',\n 'in',\n 'Wonderland',\n 'Project',\n 'Gutenberg’s',\n 'Adventures',\n 'in',\n 'Wonderland',\n 'Project',\n 'Gutenberg’s']"
     ]
    }
   ],
   "source": [
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"\n",
    "]\n",
    "\n",
    "# map() with rdd\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ae6f3f-2f2b-4fa1-90a6-adf546e829c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n('Gutenberg’s', 1)\n('Alice’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "# map() with rdd\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ca7d97-ab93-4d71-aab1-6b8230ca47ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 0)\n('Gutenberg’s', 1)\n('Alice’s', 2)\n('Adventures', 3)\n('in', 4)\n('Wonderland', 5)\n('Project', 6)\n('Gutenberg’s', 7)\n('Adventures', 8)\n('in', 9)\n('Wonderland', 10)\n('Project', 11)\n('Gutenberg’s', 12)\n"
     ]
    }
   ],
   "source": [
    "# zipWithIndex()\n",
    "rdd3 = rdd.zipWithIndex()\n",
    "for element in rdd3.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b1b57b-4b17-4371-81eb-90e7ef43f81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>firstname</th><th>lastname</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>Raj</td><td>Kumar</td><td>M</td><td>30</td></tr><tr><td>Annie</td><td>Dhumi</td><td>F</td><td>41</td></tr><tr><td>Rahul</td><td>kumar</td><td>M</td><td>62</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raj",
         "Kumar",
         "M",
         30
        ],
        [
         "Annie",
         "Dhumi",
         "F",
         41
        ],
        [
         "Rahul",
         "kumar",
         "M",
         62
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [('Raj','Kumar','M',30),\n",
    "  ('Annie','Dhumi','F',41),\n",
    "  ('Rahul','kumar','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff1e84d-5aa6-4105-bb1a-1e1c82650be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+\n|       name|gender|new_salary|\n+-----------+------+----------+\n|  Raj Kumar|     M|        35|\n|Annie Dhumi|     F|        46|\n|Rahul kumar|     M|        67|\n+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Converting DataFrame to RDD and implement map function\n",
    "rdd4 = df.rdd.map(lambda x: (x[0]+\" \"+x[1],x[2],x[3]+5))\n",
    "df1 = rdd4.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "# display(df1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5883e2-e650-47f2-9ed5-201cb45a0a18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+\n|       name|gender|new_salary|\n+-----------+------+----------+\n|  Raj Kumar|     M|        36|\n|Annie Dhumi|     F|        47|\n|Rahul kumar|     M|        68|\n+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using Column Name\n",
    "rdd5 = df.rdd.map(lambda x: (x[\"firstname\"]+\" \"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]+6))\n",
    "df2 = rdd5.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "# display(df2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2975e384-0e94-4d5f-acf1-18488cbec0e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+\n|       name|gender|new_salary|\n+-----------+------+----------+\n|  Raj Kumar|     M|        35|\n|Annie Dhumi|     F|        46|\n|Rahul kumar|     M|        67|\n+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Another Way\n",
    "rdd6 = df.rdd.map(lambda x: (x.firstname+\" \"+x.lastname, x.gender, x.salary+5))\n",
    "df3 = rdd6.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7695b9-bf42-464d-9a12-f5413da55921",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### flatMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72432e9-6e8f-4e0b-9443-0e5d2c4c8b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- PySpark flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752f61fd-d967-4aa4-af5c-d121897392d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flatmap_data = [\n",
    "    \"PySpark flatMap() is a transformation operation\" ,\"that flattens the RDD/DataFrame (array/map DataFrame columns)\", \"after applying the function\", \"on every element and returns\", \"a new PySpark RDD/DataFrame.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbae238-e4f9-48ad-bd0a-493c70e237d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark flatMap() is a transformation operation\nthat flattens the RDD/DataFrame (array/map DataFrame columns)\nafter applying the function\non every element and returns\na new PySpark RDD/DataFrame.\n"
     ]
    }
   ],
   "source": [
    "rdd7 = spark.sparkContext.parallelize(flatmap_data)\n",
    "for x in rdd7.collect():\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8301b527-8bb9-4465-a71b-3e2b0a39081e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark\nflatMap()\nis\na\ntransformation\noperation\nthat\nflattens\nthe\nRDD/DataFrame\n(array/map\nDataFrame\ncolumns)\nafter\napplying\nthe\nfunction\non\nevery\nelement\nand\nreturns\na\nnew\nPySpark\nRDD/DataFrame.\n"
     ]
    }
   ],
   "source": [
    "flatmap_rdd = rdd7.flatMap(lambda x: x.split(\" \"))\n",
    "for y in flatmap_rdd.collect():\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Map_FlatMap",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
