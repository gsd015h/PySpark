{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd2f4d89-4d7f-4dc4-80cc-96bc328dbb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Discovering Hidden PySpark Treasures: Unique Tips and Hacks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3531d096-a1fe-4ada-a91e-460732b0c452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.  Accurate Approximate Distinct Count\n",
    "---\n",
    "- Calculating the precise distinct count might take time when dealing with large datasets. To optimize this, we can use the `approx_count_distinct()` function, which provides an accurate approximation of the distinct count. This function is particularly useful when you need to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129f2ae5-cdc8-448e-bfbe-7ab3552b7bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71daea6c-0285-45e0-bade-9262c1916c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    1|\n|    3|\n|    2|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"approx-distinct-count\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (1,), (3,), (2,)], [\"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a41007f-6314-4647-b967-d4f261af6681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Distinct Count : 3\n"
     ]
    }
   ],
   "source": [
    "distinct_count = df.select(approx_count_distinct('value')).first()[0]\n",
    "print(\"Approximate Distinct Count :\", distinct_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c76b9bb-5ede-49d1-9a42-1b473bc97d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.  Optimizing UDF Performance\n",
    "---\n",
    "\n",
    "- User-Defined Function(UDFs) are powerful, but their performance can be a concern. Leveraging pandas UDFs can significantly speed up your computations by allowing you to use vectroized pandas functions on your pySpark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4178c2fe-0615-4b3c-b055-c535ca2b0fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a29b25-6a3a-4e69-92fd-dfcf1c3a24df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @pandas_udf(IntegerType())\n",
    "# def slen(s: pd.Series) -> pd.Series:\n",
    "#     return s.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb19ccc-7d3e-4f40-bfa3-26d4574fb6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n|value|result|\n+-----+------+\n|    1|   2.0|\n|    2|   4.0|\n|    3|   6.0|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Pandas-udf\").getOrCreate()\n",
    "\n",
    "@pandas_udf('double')\n",
    "def custom_udf(series: pd.Series) -> pd.Series:\n",
    "    return series * 2\n",
    "\n",
    "df = spark.createDataFrame([(1,), (2,), (3,)], ['value'])\n",
    "df.withColumn('result', custom_udf(df['value'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52358a46-62b0-439c-b1c0-47f714cfb715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.  Custom Aggregation with Pandas UDAF\n",
    "---\n",
    "Taking aggregation a step further, we can create custom aggregation functions using Pandas User-Defined Aggregate Functions (UDAF). This unlocks to perform intricate aggregations with the power of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630250dc-e826-4d8b-826b-8b6afbe52328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53191b29-624e-4a16-b5bc-0ff2612dae4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custome UDAF Result:  10\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pandas-udaf').getOrCreate()\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def custom_udaf(series: pd.Series) -> int:\n",
    "    return series.sum()\n",
    "\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,)], ['value'])\n",
    "df.show()\n",
    "result = df.agg(custom_udaf(df['value'])).first()[0]\n",
    "print('custome UDAF Result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f512c4-afcf-44a5-8cb0-070966474cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4 Dynamic Column Selection in DataFrame\n",
    "---\n",
    "Often, you may need to select columns from DataFrame based on a specific conditions. The select function can be enhanced with the `colRegex()` to achive dynamic column selection based on column name characteristics. For instance  you want to select column with names that start with 'feature_' followed by any digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb63e74-2268-4c84-88ae-46cc69f1400f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41839aed-b779-4944-b023-d18b73921bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n|feature_1|feature_2|Other|\n+---------+---------+-----+\n|        1|        2|    3|\n|        4|        5|    6|\n+---------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('columnSelection').getOrCreate()\n",
    "df = spark.createDataFrame([(1,2,3), (4,5,6)], ['feature_1', 'feature_2','Other'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f31249-5537-46cc-9653-32456ce8ed53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|feature_1|feature_2|\n+---------+---------+\n|        1|        2|\n|        4|        5|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "selected_cols = df.select(df.colRegex('`feature_[123]`'))\n",
    "selected_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b68866-fea0-41f4-99d5-c664da27ba79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.  Efficient Data Sampling (Stratified Sampling)\n",
    "---\n",
    "\n",
    "Sampling data is a common technique for exploratory analysis. PySpark offers the `sampleBy()` function to sample data based on a specified condition. This function is more efficient than sample function. `sampleBy()` function allows you to maintain the proportion of classes within your sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605ab529-050c-4a39-bbec-54efba174da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|Value|Class|\n+-----+-----+\n|    1|    A|\n|    2|    B|\n|    3|    A|\n|    4|    B|\n|    5|    A|\n|    6|    B|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Stratified_sample').getOrCreate()\n",
    "df = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'A'), (4, 'B'), (5, 'A'), (6, 'B')], ['Value', 'Class'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a049f1ac-da11-4e69-af52-ececf2f32279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|Value|Class|\n+-----+-----+\n|    4|    B|\n|    6|    B|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sampleBy('Class', fractions={'A':0.3, 'B':0.7}, seed=42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9962f97e-5397-4a4d-a69f-2e32951a1e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Advance window Function\n",
    "---\n",
    "Window function are a staple in data analysis, but PySpark's window functions offer advanced capabilities. The `percent_rank()` function, for instance, calculate the relative rank of each row within a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ae6890-90bc-4e79-b021-4f0dab0aac8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e26a92-b00c-4682-b5c4-22374fb7b160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|value|class|\n+-----+-----+\n|    1|    A|\n|    2|    B|\n|    3|    A|\n|    4|    A|\n|    5|    A|\n|    6|    B|\n|    7|    B|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Window-Function').getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (3, \"A\"), (4, \"A\"), (5, \"A\"), (6, \"B\"), (7, \"B\")], [\"value\", \"class\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33112d37-7e62-4185-b8eb-4a1c602257a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+\n|value|class|      percent_rank|\n+-----+-----+------------------+\n|    1|    A|               0.0|\n|    3|    A|0.3333333333333333|\n|    4|    A|0.6666666666666666|\n|    5|    A|               1.0|\n|    2|    B|               0.0|\n|    6|    B|               0.5|\n|    7|    B|               1.0|\n+-----+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window_spaec = Window.partitionBy('class').orderBy('value')\n",
    "result_df = df.withColumn('percent_rank', percent_rank().over(window_spaec))\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e515e948-70fd-4803-8a60-0021dd8a08b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Broadcasting Small DataFrames\n",
    "---\n",
    "When joining a large DataFrame with a small one. Pyspark `broadcast()` function can significantly speed up the process by distributing the small DataFrame to all worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fccfc489-5ce1-42bf-bf1e-ae3115abec31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5091d1d9-8ead-4d2a-97bf-85a278a5d5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('BroadcastJoin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b174cfb9-7f6d-4460-85f2-89bd804d0df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|state_code|\n+---+----------+\n|  1|        CA|\n|  2|        FL|\n|  3|        NY|\n+---+----------+\n\n+---+----------+\n| id|    states|\n+---+----------+\n|  1|California|\n|  2|   Florida|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "large_df = spark.createDataFrame([(1, \"CA\"), (2, \"FL\"), (3, \"NY\")], [\"id\", \"state_code\"])\n",
    "large_df.show()\n",
    "small_df = spark.createDataFrame([(1, \"California\"), (2, \"Florida\")], [\"id\", \"states\"])\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a322305-a075-4b42-a5f6-17f593b3be34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n| id|state_code|    states|\n+---+----------+----------+\n|  1|        CA|California|\n|  2|        FL|   Florida|\n|  3|        NY|      null|\n+---+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_df = large_df.join(broadcast(small_df), on=\"id\", how=\"left\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_UniqueTipsandHacks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
